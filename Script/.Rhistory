pred[pred < min(train$price_doc)] = min(train$price_doc)
write.csv(cbind(id = rownames(test),
price_doc = pred),
"impute_clean_xgboost_eval.csv", row.names = F)
dim(train)
dim(test)
dim(read.csv("../Data/train.csv"))
dim(read.csv("../Data/test.csv"))
summary(rownames(test))
head(rownames(test))
tail(rownames(test))
anyDuplicated(rownames(test))
head(read.csv("../Data/test.csv")$id)
tail(read.csv("../Data/test.csv")$id)
tail(read.csv("../Data/test_new.csv")$id)
train = read.csv("../Data/train_new.csv", stringsAsFactors = T)
test = read.csv("../Data/test_new.csv", stringsAsFactors = T)
rownames(train) = train$id
rownames(test) = test$id
train = train[,-1]
test = test[,-1]
train_price = train[,ncol(train)]
macro = read.csv("../Data/macro_impute.csv")
total = rbind(train[,-ncol(train)], test)
total = merge(total, macro, by = "timestamp")
total = total[,-1]
colTypes = sapply(1:ncol(total), function(i) class(total[,i]))
total_factors = total[, colTypes == "factor"]
for(i in 1:ncol(total_factors)) {
t = as.character(total_factors[,i], levels = unique(total_factors[,i]))
t[is.na(t)] = "NA"
total_factors[,i] = factor(t, levels = unique(t))
}
len_factor = apply(total_factors, 2, function(x) length(unique(x)))
total_factors$name =NULL
total_dummy = model.matrix(~0+., total_factors)
train = total[1:nrow(train), colTypes != "factor"]
train = cbind(train, total_dummy[1:nrow(train),])
train = cbind(train, price_doc = train_price)
test = total[-c(1:nrow(train)), colTypes != "factor"]
test = cbind(test, total_dummy[-c(1:nrow(train)),])
tail(rownames(test))
tail(rownames(total))
train = read.csv("../Data/train_new.csv", stringsAsFactors = T)
test = read.csv("../Data/test_new.csv", stringsAsFactors = T)
rownames(train) = train$id
rownames(test) = test$id
train = train[,-1]
test = test[,-1]
tail(rownames(test))
macro = read.csv("../Data/macro_impute.csv")
total = rbind(train[,-ncol(train)], test)
total = merge(total, macro, by = "timestamp")
total = total[,-1]
train_price = train[,ncol(train)]
tail(rownames(total))
dim(total)
dim(train)
dim(test)
nrow(test) + nrow(train)
tail(rownames(train))
head(rownames(test))
rownames(total)[1:nrow(train)] = rownames(train)
anyDuplicated(rownames(train))
length(rownames(train))
nrow(train)
rownames(total) = c(rownames(train), rownames(test))
trainId = rownames(train)
testId = rownames(test)
colTypes = sapply(1:ncol(total), function(i) class(total[,i]))
total_factors = total[, colTypes == "factor"]
for(i in 1:ncol(total_factors)) {
t = as.character(total_factors[,i], levels = unique(total_factors[,i]))
t[is.na(t)] = "NA"
total_factors[,i] = factor(t, levels = unique(t))
}
len_factor = apply(total_factors, 2, function(x) length(unique(x)))
total_factors$name =NULL
total_dummy = model.matrix(~0+., total_factors)
train = total[1:nrow(train), colTypes != "factor"]
train = total[trainId, colTypes != "factor"]
train = cbind(train, total_dummy[trainId,])
train = cbind(train, price_doc = train_price)
test = total[testId, colTypes != "factor"]
test = cbind(test, total_dummy[testId,])
tail(rownames(test))
rm(total)
rm(total_dummy)
rm(total_factors)
rm(macro)
set.seed(88)
split = sample.split(train$price_doc, SplitRatio = 0.7)
datTest = train[!split,]
datTrain = train[split,]
ef = function(pred, real) {
pred[is.na(pred)] = mean(real)
pred[pred < 0] = min(real)
return(sqrt(sum((log(pred+1) - log(real + 1))^2)/length(pred)))
}
model1 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear",
eval_metric = "rmse", max_depth = 6, eta = 0.3,
nthread = 8, nrounds = 1000, subsample = 0.7, colsample_bytree = 0.7)
# pred <- predict(model, dtest)
ef(predict(model1,  as.matrix(datTest)),  datTest$price_doc)
pred = predict(model1, as.matrix(test))
pred[pred < min(train$price_doc)] = min(train$price_doc)
write.csv(cbind(id = rownames(test),
price_doc = pred),
"impute_clean_xgboost_eval.csv", row.names = F)
lc = as.data.frame(matrix(0, nrow = 10, ncol = 3))
colnames(lc) = c("size", "TrainRMSE", "TestRMSE")
lc$size = ceiling(2.732^(1:10))
set.seed(5354)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
for(i in 1:nrow(lc)) {
temptrain = datTrain[rand_ind[1:lc[i,1]],]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 1, nthread = 8,
eval_metric = "rmse", nrounds = 2000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7)
lc[i,2] = ef(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = ef(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
lc = as.data.frame(matrix(0, nrow = 10, ncol = 3))
colnames(lc) = c("size", "TrainRMSE", "TestRMSE")
lc$size = ceiling(2.732^(1:10))
set.seed(5354)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
for(i in 1:nrow(lc)) {
temptrain = datTrain[rand_ind[1:lc[i,1]],]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 1, nthread = 8,
eval_metric = "rmse", nrounds = 500, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7)
lc[i,2] = ef(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = ef(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
ggplot(lc) +
geom_line(aes(x = size, y = TrainRMSE), color = "red") +
geom_line(aes(x = size, y = TestRMSE), color = "blue") +
geom_hline(yintercept = 0.32, color = "black")
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.01, 0.02, 0.05),
max_depth = c(4, 5, 6),
gamma = c(3, 1, 0.3, 0.1),
colsample_bytree = c(0.75, 1),
min_child_weight = c(1,5),
subsample = c(0.7, 0.8)
)
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 3,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",                   # save losses across all models
classProbs = TRUE,                      # set to TRUE for AUC to be computed
allowParallel = TRUE
)
xgb_train_1 = train(
x = datTrain[,-ncol(datTrain)],
y = datTrain[,ncol(datTrain)],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree",
eval_metric = "rmse",
objective = "reg:linear",
silent = 1,
nthread = 8
)
warnings()
head(xgb_train_1$results[order(xgb_train_1$results$RMSE),])
ef(predict(model1,  as.matrix(datTest)),  datTest$price_doc)
model2 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear",
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.3,
colsample_bytree = 1, min_child_weight = 1,
subsample = 0.8)
model2 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear", nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.3,
colsample_bytree = 1, min_child_weight = 1,
subsample = 0.8)
ef(predict(model2,  as.matrix(datTest)),  datTest$price_doc)
pred2 = predict(model2, as.matrix(test))
summary(pred2)
write.csv(cbind(id = rownames(test),
price_doc = pred2),
"impute_clean_tune_xgboost_eval.csv", row.names = F)
head(xgb_train_1$results[order(xgb_train_1$results$RMSE),], 10)
dim(xgb_grid_1)
dim(expand.grid(
nrounds = 1000,
eta = c(0.015, 0.02, 0.025),
max_depth = c(4, 5, 6),
gamma = c(3, 1, 0.3),
colsample_bytree = c(0.75,0.85, 1),
min_child_weight = c(1,2,5),
subsample = c(0.6, 0.7, 0.8)
))
xgb_train_1
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.015, 0.02, 0.025),
max_depth = c(4, 5, 6),
gamma = c(3, 1, 0.3),
colsample_bytree = c(0.75,0.85, 1),
min_child_weight = c(1, 2, 5),
subsample = c(0.6, 0.7, 0.8)
)
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 3,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",                   # save losses across all models
classProbs = TRUE,                      # set to TRUE for AUC to be computed
allowParallel = TRUE
)
xgb_train_1 = train(
x = datTrain[,-ncol(datTrain)],
y = datTrain[,ncol(datTrain)],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree",
eval_metric = "rmse",
objective = "reg:linear",
silent = 1,
nthread = 8
)
save.image("./impute_clean_xgboost.RData")
head(xgb_train_1$results[order(xgb_train_1$results$RMSE),])
model2 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear", nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.025, gamma = 0.3, colsample_bytree = 0.85,
min_child_weight = 1, subsample = 0.88)
ef(predict(model2,  as.matrix(datTest)),  datTest$price_doc)
pred2 = predict(model2, as.matrix(test))
summary(pred2)
write.csv(cbind(id = rownames(test),
price_doc = pred2),
"impute_clean_tune_xgboost_eval.csv", row.names = F)
save.image("./impute_clean_xgboost.RData")
library(doSNOW)
library(foreach)
library(caTools)
# library(caret)
# library(FNN)
# library(KernelKnn)
# library(e1071)
library(doSNOW)
library(foreach)
train = read.csv("../Data/train_new.csv", stringsAsFactors = T)
test = read.csv("../Data/test_new.csv", stringsAsFactors = T)
rownames(train) = train$id
rownames(test) = test$id
train = train[,-1]
test = test[,-1]
train_price = train[,ncol(train)]
macro = read.csv("../Data/macro_impute.csv")
total = rbind(train[,-ncol(train)], test)
total = merge(total, macro, by = "timestamp")
total = total[,-1]
rownames(total) = c(rownames(train), rownames(test))
trainId = rownames(train)
testId = rownames(test)
colTypes = sapply(1:ncol(total), function(i) class(total[,i]))
total_factors = total[, colTypes == "factor"]
for(i in 1:ncol(total_factors)) {
t = as.character(total_factors[,i], levels = unique(total_factors[,i]))
t[is.na(t)] = "NA"
total_factors[,i] = factor(t, levels = unique(t))
}
len_factor = apply(total_factors, 2, function(x) length(unique(x)))
total_factors$name =NULL
total_dummy = model.matrix(~0+., total_factors)
train = total[trainId, colTypes != "factor"]
# col_na = apply(train, 2, anyNA)
# na_mean = colMeans(train[,col_na], na.rm = T)
for(i in 1:ncol(train)) {
na_row = is.na(train[,i])
if(sum(na_row) > 0) {
train[na_row, i] = mean(train[,i], na.rm = T)
}
}
train = cbind(train, total_dummy[trainId,])
train = cbind(train, price_doc = train_price)
test = total[testId, colTypes != "factor"]
for(i in 1:ncol(test)) {
na_row = is.na(test[,i])
if(sum(na_row) > 0) {
test[na_row, i] = mean(train[,i], na.rm = T)
}
}
test = cbind(test, total_dummy[testId,])
stopifnot(!(anyNA(test) | anyNA(train)))
# rm(total)
# rm(total_dummy)
# rm(total_factors)
# rm(macro)
set.seed(88)
split = sample.split(train$price_doc, SplitRatio = 0.7)
datTest = train[!split,]
datTrain = train[split,]
cl = makeCluster(6)
registerDoSNOW(cl)
dim(datTrain)
set.seed(847)
split1 = sample.split(datTrain$price_doc, SplitRatio = 0.67)
split2 = sample.split(datTrain$price_doc[!split1], SplitRatio = 0.5)
length(split1)
table(split1)
folds= list()
folds[[1]] = datTrain[!split1,]
folds[[2]] = datTrain[split1,][split2,]
folds[[3]] = datTrain[split1,][!split2,]
ef = function(pred, real) {
pred[is.na(pred)] = mean(real)
pred[pred < 0] = min(real)
return(sqrt(sum((log(pred+1) - log(real + 1))^2)/length(pred)))
}
mean(3,4,5)
mean(c(3,4,5))
cv_res = foreach(i=seq(2,20,2), .packages=c('caret'),
.verbose = T, .combine = rbind) %dopar%  {
td = rbind(folds[[1]],folds[[2]])
vd = folds[[3]]
fit = knnreg(x = td[,-ncol(td)], y =  td[,ncol(td)], k = i)
e1 = ef(predict(fit, vd[,-ncol(vd)]), vd[,ncol(vd)])
td = rbind(folds[[1]],folds[[3]])
vd = folds[[2]]
fit = knnreg(x = td[,-ncol(td)], y =  td[,ncol(td)], k = i)
e2 = ef(predict(fit, vd[,-ncol(vd)]), vd[,ncol(vd)])
td = rbind(folds[[3]],folds[[2]])
vd = folds[[1]]
fit = knnreg(x = td[,-ncol(td)], y =  td[,ncol(td)], k = i)
e3 = ef(predict(fit, vd[,-ncol(vd)]), vd[,ncol(vd)])
return(c(i, mean(c(e1,e2,e3))))
}
expand.grid
cv_grid = expand.grid(fold = 1:3,
k = seq(2,20,2))
cv_grid
library(caTools)
library(caret)
# library(FNN)
# library(KernelKnn)
# library(e1071)
library(doSNOW)
library(foreach)
train = read.csv("../Data/train_new.csv", stringsAsFactors = T)
test = read.csv("../Data/test_new.csv", stringsAsFactors = T)
rownames(train) = train$id
rownames(test) = test$id
train = train[,-1]
test = test[,-1]
train_price = train[,ncol(train)]
macro = read.csv("../Data/macro_impute.csv")
total = rbind(train[,-ncol(train)], test)
total = merge(total, macro, by = "timestamp")
total = total[,-1]
rownames(total) = c(rownames(train), rownames(test))
trainId = rownames(train)
testId = rownames(test)
colTypes = sapply(1:ncol(total), function(i) class(total[,i]))
total_factors = total[, colTypes == "factor"]
for(i in 1:ncol(total_factors)) {
t = as.character(total_factors[,i], levels = unique(total_factors[,i]))
t[is.na(t)] = "NA"
total_factors[,i] = factor(t, levels = unique(t))
}
len_factor = apply(total_factors, 2, function(x) length(unique(x)))
total_factors$name =NULL
total_dummy = model.matrix(~0+., total_factors)
train = total[trainId, colTypes != "factor"]
# col_na = apply(train, 2, anyNA)
# na_mean = colMeans(train[,col_na], na.rm = T)
for(i in 1:ncol(train)) {
na_row = is.na(train[,i])
if(sum(na_row) > 0) {
train[na_row, i] = mean(train[,i], na.rm = T)
}
}
train = cbind(train, total_dummy[trainId,])
train = cbind(train, price_doc = train_price)
test = total[testId, colTypes != "factor"]
for(i in 1:ncol(test)) {
na_row = is.na(test[,i])
if(sum(na_row) > 0) {
test[na_row, i] = mean(train[,i], na.rm = T)
}
}
test = cbind(test, total_dummy[testId,])
stopifnot(!(anyNA(test) | anyNA(train)))
# rm(total)
# rm(total_dummy)
# rm(total_factors)
# rm(macro)
set.seed(88)
split = sample.split(train$price_doc, SplitRatio = 0.7)
datTest = train[!split,]
datTrain = train[split,]
cl = makeCluster(6)
registerDoSNOW(cl)
ef = function(pred, real) {
pred[is.na(pred)] = mean(real)
pred[pred < 0] = min(real)
return(sqrt(sum((log(pred+1) - log(real + 1))^2)/length(pred)))
}
# fit_cv_pair1 = KernelKnnCV(data = datTrain[,-ncol(datTrain)],
#                            y = datTrain[,ncol(datTrain)], k = 10 ,
#                            folds = 3, regression = T,
#                            threads = 1)
# cv_knn = tune(knnreg, train.x = datTrain[,-ncol(datTrain)],
#               train.y = datTrain[,ncol(datTrain)], k = seq(2, 20, 2),
#               tunecontrol=tune.control(sampling = "cross", cross=3))
#
# fit_knn = knnreg(x = datTrain[,-ncol(datTrain)], y = datTrain[,ncol(datTrain)], k = 10)
#
# predict(fit_knn, datTest)
set.seed(847)
split1 = sample.split(datTrain$price_doc, SplitRatio = 0.67)
split2 = sample.split(datTrain$price_doc[!split1], SplitRatio = 0.5)
folds= list()
folds[[1]] = datTrain[!split1,]
folds[[2]] = datTrain[split1,][split2,]
folds[[3]] = datTrain[split1,][!split2,]
cv_grid = expand.grid(fold = 1:3,
k = seq(2,20,2))
cv_res = foreach(i= 1:nrow(cv_grid), .packages=c('caret'),
.verbose = T, .combine = rbind) %dopar%  {
f = cv_grid[i,1]
ck = cv_grid[i,2]
vd = folds[[f]]
td = rbind(folds[[ifelse(k+1 > 3, (k+1)%%3, k+1)]],
folds[[ifelse(k+2 > 3, (k+2)%%3, k+2)]])
fit = knnreg(x = td[,-ncol(td)], y =  td[,ncol(td)], k = i)
error = ef(predict(fit, vd[,-ncol(vd)]), vd[,ncol(vd)])
return(c(f, ck, error))
}
cv_res = foreach(i= 1:nrow(cv_grid), .packages=c('caret'),
.verbose = T, .combine = rbind) %dopar%  {
f = cv_grid[i,1]
ck = cv_grid[i,2]
vd = folds[[f]]
td = rbind(folds[[ifelse(f+1 > 3, (f+1)%%3, f+1)]],
folds[[ifelse(f+2 > 3, (f+2)%%3, f+2)]])
fit = knnreg(x = td[,-ncol(td)], y =  td[,ncol(td)], k = ck)
error = ef(predict(fit, vd[,-ncol(vd)]), vd[,ncol(vd)])
return(c(f, ck, error))
}
cv_res
library(dplyr)
colnames(cv_res) = c("Fold", "k", "error")
cv_summary = cv_res %>%
group_by(k) %>%
summarise(mean = mean(error), sd = sd(error))
cv_summary = as.data.frame(cv_res) %>%
group_by(k) %>%
summarise(mean = mean(error), sd = sd(error))
cv_summary
library(ggplot2)
ggplot(cv_summary, aes(x = k, y = mean)) + geom_line() + geom_point()
set.seed(847)
split1 = sample.split(datTrain$price_doc, SplitRatio = 0.67)
split2 = sample.split(datTrain$price_doc[!split1], SplitRatio = 0.5)
folds= list()
folds[[1]] = datTrain[!split1,]
folds[[2]] = datTrain[split1,][split2,]
folds[[3]] = datTrain[split1,][!split2,]
cv_grid = expand.grid(fold = 1:3,
k = seq(4,30,2))
cv_res = foreach(i= 1:nrow(cv_grid), .packages=c('caret'),
.verbose = T, .combine = rbind) %dopar%  {
f = cv_grid[i,1]
ck = cv_grid[i,2]
vd = folds[[f]]
td = rbind(folds[[ifelse(f+1 > 3, (f+1)%%3, f+1)]],
folds[[ifelse(f+2 > 3, (f+2)%%3, f+2)]])
fit = knnreg(x = td[,-ncol(td)], y =  td[,ncol(td)], k = ck)
error = ef(predict(fit, vd[,-ncol(vd)]), vd[,ncol(vd)])
return(c(f, ck, error))
}
colnames(cv_res) = c("Fold", "k", "error")
cv_summary = as.data.frame(cv_res) %>%
group_by(k) %>%
summarise(mean = mean(error), sd = sd(error))
ggplot(cv_summary, aes(x = k, y = mean)) + geom_line() + geom_point()
save.image("./impute_clean_knn.RData")
cv_summary
bestk = 16
fit = knnreg(x = datTrain[,-ncol(datTrain)], y =  datTrain[,ncol(datTrain)], k = bestk)
error = ef(predict(fit, datTest[,-ncol(datTest)]), datTest[,ncol(datTest)])
pred = predict(fit, test)
write.csv(cbind(id = rownames(test),
price_doc = pred),
"impute_clean_tune_knn_k16_eval.csv", row.names = F)
error
summary(pred)
save.image("./impute_clean_knn.RData")
