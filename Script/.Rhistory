fit <- glm(price_doc ~ .,data=datTrain)
sf = summary(fit)
pred = predict(fit, test)
head(pred)
test[1:5,1:5]
summary(pred)
write.csv(cbind(id = test$id,
price_doc = pred),
"baseline_eval.csv", row.names = F)
pred[pred < 0] = 0
pred = predict(fit, test)
pred[is.na(pred)] = 0
pred[pred < 0] = 0
write.csv(cbind(id = test$id,
price_doc = pred),
"baseline_eval.csv", row.names = F)
?rpart
train = read.csv("../Data/train.csv")
rownames(train) = train$id
train = train[,-1]
# train$timestamp = factor(gsub("-([[:digit:]]{1,})$","",train$timestamp))
test = read.csv("../Data/test.csv")
# test$timestamp = factor(gsub("-([[:digit:]]{1,})$","",test$timestamp))
# train = train[,c(colnames(test), "price_doc")]
# ind_na = apply(train, 2, function(x) any(is.na(x)))
# train_noNA = train[,!ind_na]
train_noNA = train[,-1]
# test = test[,ind_na]
set.seed(88)
split = sample.split(train_noNA$price_doc, SplitRatio = 0.7)
datTest = train_noNA[!split,]
# datTest$timestamp = factor(datTest$timestamp, levels = levels(train$timestamp))
datTrain = train_noNA[split,]
# datTrain$timestamp = factor(datTrain$timestamp, levels = levels(train$timestamp))
tree = rpart(formula = price_doc ~ ., data=datTrain,
parms = list(split  = "information"), method="anova",
control = rpart.control(minsplit = 50))
pred1 = predict(fit, datTest)
return(sqrt(sum((log(pred+1) - log(real + 1))^2)/length(pred)))
ef = function(pred, real) {
return(sqrt(sum((log(pred+1) - log(real + 1))^2)/length(pred)))
}
ef(predict(fit, datTest),  datTest$price_doc)
ef = function(pred, real) {
pred[is.na(pred)] = mean(real)
return(sqrt(sum((log(pred+1) - log(real + 1))^2)/length(pred)))
}
ef(predict(fit, datTest),  datTest$price_doc)
summary(predict(fit, datTest))
ef = function(pred, real) {
pred[is.na(pred)] = mean(real)
pred[pred < 0] = min(real)
return(sqrt(sum((log(pred+1) - log(real + 1))^2)/length(pred)))
}
ef(predict(fit, datTest),  datTest$price_doc)
pred = predict(fit, test)
pred[is.na(pred)] = mean(train$price_doc)
pred[pred < 0] = min(train$price_doc)
write.csv(cbind(id = test$id,
price_doc = pred),
"baseline_tree_eval.csv", row.names = F)
plotcp(tree)
treePrune<- prune(tree, cp= tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"])
ef(predict(treePrune, datTest),  datTest$price_doc)
ef(predict(fit, datTest),  datTest$price_doc)
pred = predict(fit, test)
pred[is.na(pred)] = mean(train$price_doc)
pred[pred < 0] = min(train$price_doc)
write.csv(cbind(id = test$id,
price_doc = pred),
"baseline_tree_eval.csv", row.names = F)
pred = predict(treePrune, test)
pred[is.na(pred)] = mean(train$price_doc)
pred[pred < 0] = min(train$price_doc)
write.csv(cbind(id = test$id,
price_doc = pred),
"baseline_treePrune_eval.csv", row.names = F)
library(caTools)
library(caret)
library(xgboost)
# library(mice)
# library(VIM)
train = read.csv("../Data/train.csv", stringsAsFactors = T)
test = read.csv("../Data/test.csv", stringsAsFactors = T)
rownames(train) = train$id
rownames(test) = test$id
train = train[,-1]
test = test[,-1]
## no time stamp
train = train[,-1]
test = test[,-1]
total = rbind(train[,-ncol(train)], test)
colNA = apply(total,2, anyNA)
## NO NA Model
total_noNA = total[,!colNA]
colTypes = sapply(1:ncol(total_noNA), function(i) class(total_noNA[,i]))
ind_factor = which(colTypes == "factor")
total_noNA = cbind(total_noNA[, -ind_factor],
model.matrix(~0+., total_noNA[, ind_factor]))
train_noNA = total_noNA[rownames(train),]
train_noNA$price_doc = train$price_doc
test_noNA = total_noNA[rownames(test),]
# test = test[,ind_na]
set.seed(88)
split = sample.split(train_noNA$price_doc, SplitRatio = 0.7)
datTest = train_noNA[!split,]
# datTest$timestamp = factor(datTest$timestamp, levels = levels(train$timestamp))
datTrain = train_noNA[split,]
# datTrain$timestamp = factor(datTrain$timestamp, levels = levels(train$timestamp))
ef = function(pred, real) {
pred[is.na(pred)] = mean(real)
pred[pred < 0] = min(real)
return(sqrt(sum((log(pred+1) - log(real + 1))^2)/length(pred)))
}
### Tune
### Tune
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.5, 0.1, 0.02),
max_depth = c(2, 5, 10),
gamma = c(10, 3, 1, 0.3, 0.1),
colsample_bytree = c(0.5, 0.75, 1),
min_child_weight = seq(1, 11, 4),
subsample = seq(0.4, 1, 0.3)
)
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 3,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",                   # save losses across all models
classProbs = TRUE,                      # set to TRUE for AUC to be computed
allowParallel = TRUE
)
xgb_train_1 = train(
x = datTrain[,-ncol(datTrain)],
y = datTrain[,ncol(datTrain)],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree",
eval_metric = "rmse",
objective = "reg:linear",
silent = 1,
nthread = 8
)
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.5, 0.1, 0.02),
max_depth = c(2, 5, 8),
gamma = c(10, 3, 1, 0.3, 0.1),
colsample_bytree = c(0.5, 0.75, 1),
min_child_weight = seq(1, 11, 4),
subsample = seq(0.4, 1, 0.3)
)
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 3,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",                   # save losses across all models
classProbs = TRUE,                      # set to TRUE for AUC to be computed
allowParallel = TRUE
)
xgb_train_1 = train(
x = datTrain[,-ncol(datTrain)],
y = datTrain[,ncol(datTrain)],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree",
eval_metric = "rmse",
objective = "reg:linear",
silent = 1,
nthread = 8
)
?xgboost
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.5, 0.1, 0.02),
max_depth = c(2, 5, 8),
gamma = c(3, 1, 0.3, 0.1),
colsample_bytree = c(0.5, 0.75, 1),
min_child_weight = seq(1, 11, 4),
subsample = c(0.7, 1)
)
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 3,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",                   # save losses across all models
classProbs = TRUE,                      # set to TRUE for AUC to be computed
allowParallel = TRUE
)
xgb_train_1 = train(
x = datTrain[,-ncol(datTrain)],
y = datTrain[,ncol(datTrain)],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree",
eval_metric = "rmse",
objective = "reg:linear",
silent = 1,
nthread = 8
)
save.image("./tune_xgboost/RData")
save.image("C:/XIANGGU/Socket/SRHM/Script/tune_xgboost.RData")
xgb_train_1
xgb_train_1$results
dim(xgb_train_1$results)
head(xgb_train_1$results[order(xgb_train_1$results$RMSE),])
model1 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear",
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.1,
colsample_bytree = 1, min_child_weight = 1, subsample = 0.7)
head(xgb_train_1$results[order(xgb_train_1$results$RMSE),])
model1 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear", silent = 1,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.1,
colsample_bytree = 1, min_child_weight = 1, subsample = 0.7)
model1 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear", silent = 2,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.1,
colsample_bytree = 1, min_child_weight = 1, subsample = 0.7)
model1 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear", silent = 2, nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.1,
colsample_bytree = 1, min_child_weight = 1, subsample = 0.7)
ef(predict(model1, dtest),  datTest$price_doc)
ef(predict(model1, datTest),  datTest$price_doc)
ef(predict(model1, as.matrix(datTest)),  datTest$price_doc)
pred = predict(model1, as.matrix(test_noNA))
write.csv(cbind(id = rownames(test_noNA),
price_doc = pred),
"noNA_xgboost_tune1_eval.csv", row.names = F)
model2 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear", silent = 2, nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 3,
colsample_bytree = 0.5, min_child_weight = 5, subsample = 0.7)
ef(predict(model2, as.matrix(datTest)),  datTest$price_doc)
pred = predict(model2, as.matrix(test_noNA))
write.csv(cbind(id = rownames(test_noNA),
price_doc = pred),
"noNA_xgboost_tune2_eval.csv", row.names = F)
ef(predict(model1, as.matrix(datTest)),  datTest$price_doc)
head(xgb_train_1$results[order(xgb_train_1$results$RMSE),], 10)
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.01, 0.02, 0.05),
max_depth = c(4, 5, 6),
gamma = c(3, 1, 0.3, 0.1),
colsample_bytree = c(0.75, 1),
min_child_weight = c(1,5),
subsample = c(0.7, 0.8)
)
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 3,
verboseIter = TRUE,
returnData = FALSE,
returnResamp = "all",                   # save losses across all models
classProbs = TRUE,                      # set to TRUE for AUC to be computed
allowParallel = TRUE
)
xgb_train_1 = train(
x = datTrain[,-ncol(datTrain)],
y = datTrain[,ncol(datTrain)],
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbTree",
eval_metric = "rmse",
objective = "reg:linear",
silent = 1,
nthread = 8
)
save.image("./tune2_xgboost.RData")
head(xgb_train_1$results[order(xgb_train_1$results$RMSE),])
head(xgb_train_1$results[order(xgb_train_1$results$RMSE),], 10)
model2 = xgboost(data = as.matrix(datTrain[,-ncol(datTrain)]),
label = datTrain[,ncol(datTrain)],
objective = "reg:linear", silent = 2, nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 6,
eta = 0.01, gamma = 1, colsample_bytree = 0.75,
min_child_weight = 1, subsample = 0.7)
ef(predict(model2, as.matrix(datTest)),  datTest$price_doc)
pred = predict(model2, as.matrix(test_noNA))
write.csv(cbind(id = rownames(test_noNA),
price_doc = pred),
"noNA_xgboost_tune2_eval.csv", row.names = F)
dim(datTrain)
lc = as.data.frame(matrix(0, nrow = 10, ncol = 3))
lc
colnames(lc) = c("size", "TrainRMSE", "TestRMSE")
23165/10
log10(23165)
4^(1:10)
23165^(1/10)
3^(1:10)
2.7^(1:10)
2.73^(1:10)
ceiling(2.73^(1:10))
ceiling(2.732^(1:10))
lc$size = ceiling(2.732^(1:10))
lc
sample(3, 1:10)
sample(1:10)
sample(1:10, 3)
sample(1:10, 3, replace = F)
sample(1:10, 3, replace = F)
for(i in 1:nrow(lc)) {
temptrain = datTrain[sample(1:nrow(datTrain), lc[i,1]),]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 2, nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.1,
colsample_bytree = 1, min_child_weight = 1, subsample = 0.7)
lc[i,2] = ef(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = ef(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
lcm
lc
summary(predict(lcm, as.matrix(temptrain)))
summary(predict(lcm, as.matrix(temptrain)) < 0)
pred
summary(pred)
efrmse = function(pred, real) {
pred[is.na(pred)] = mean(real)
pred[pred < 0] = min(real)
return(sqrt(sum((pred - real)^2)/length(pred)))
}
efrmse = function(pred, real) {
pred[is.na(pred)] = mean(real)
pred[pred < 0] = min(real)
return(sqrt(sum((pred - real)^2)/length(pred)))
}
lc = as.data.frame(matrix(0, nrow = 10, ncol = 3))
colnames(lc) = c("size", "TrainRMSE", "TestRMSE")
lc$size = ceiling(2.732^(1:10))
for(i in 1:nrow(lc)) {
temptrain = datTrain[sample(1:nrow(datTrain), lc[i,1]),]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 2, nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7)
lc[i,2] = efrmse(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = efrmse(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
lc
?sample
sample(1:nrow(datTrain), lc[i,1], replace = F)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
anyDuplicated(rand_ind)
lc = as.data.frame(matrix(0, nrow = 10, ncol = 3))
colnames(lc) = c("size", "TrainRMSE", "TestRMSE")
lc$size = ceiling(2.732^(1:10))
set.seed(934984)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
for(i in 1:nrow(lc)) {
temptrain = datTrain[rand_ind[1:lc[i,1]],]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 2, nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7)
lc[i,2] = efrmse(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = efrmse(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
lc
?xgboost
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 1, nthread = 8,
eval_metric = "rmse", nrounds = 1000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7, verbose = 0)
efrmse(rep(0,3),  datTrain[rand_ind[1:lc[1,1]],]$price_doc)
efrmse(rep(0,153),  datTrain[rand_ind[1:lc[5,1]],]$price_doc)
lcm
lcm$niter
lcm$evaluation_log
class(lcm$evaluation_log)
as.data.frame(lcm$evaluation_log)
ggplot(as.data.frame(lcm$evaluation_log))
ggplot(as.data.frame(lcm$evaluation_log)) + geom_line(ase(x=iter, y = train_rmse))
ggplot(as.data.frame(lcm$evaluation_log)) + geom_line(aes(x=iter, y = train_rmse))
efrmse(rep(mean(datTrain[rand_ind[1:lc[5,1]],]$price_doc),153),  datTrain[rand_ind[1:lc[5,1]],]$price_doc)
lc
ggplot(lc) + geom_line(aes(x = size, y = TrainRMSE), color = "red") + geom_line(aes(x = size, y = TestRMSE), color = "blue")
lc = as.data.frame(matrix(0, nrow = 10, ncol = 3))
colnames(lc) = c("size", "TrainRMSE", "TestRMSE")
lc$size = ceiling(2.732^(1:10))
set.seed(934984)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
for(i in 1:nrow(lc)) {
temptrain = datTrain[rand_ind[1:lc[i,1]],]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 1, nthread = 8,
eval_metric = "rmse", nrounds = 5000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7, verbose = 0)
lc[i,2] = ef(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = ef(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
set.seed(934984)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
for(i in 1:nrow(lc)) {
temptrain = datTrain[rand_ind[1:lc[i,1]],]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 1, nthread = 8,
eval_metric = "rmse", nrounds = 5000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7, verbose = 0)
lc[i,2] = ef(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = ef(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
lc = as.data.frame(matrix(0, nrow = 10, ncol = 3))
colnames(lc) = c("size", "TrainRMSE", "TestRMSE")
lc$size = ceiling(2.732^(1:10))
set.seed(934984)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
for(i in 1:nrow(lc)) {
temptrain = datTrain[rand_ind[1:lc[i,1]],]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 1, nthread = 8,
eval_metric = "rmse", nrounds = 5000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7)
lc[i,2] = ef(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = ef(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
lc = as.data.frame(matrix(0, nrow = 10, ncol = 3))
colnames(lc) = c("size", "TrainRMSE", "TestRMSE")
lc$size = ceiling(2.732^(1:10))
set.seed(934984)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
for(i in 1:nrow(lc)) {
temptrain = datTrain[rand_ind[1:lc[i,1]],]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 1, nthread = 8,
eval_metric = "rmse", nrounds = 2000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7)
lc[i,2] = ef(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc[i,3] = ef(predict(lcm, as.matrix(datTest)),  datTest$price_doc)
}
lc
ggplot(lc) +
geom_line(aes(x = size, y = TrainRMSE), color = "red") +
geom_line(aes(x = size, y = TestRMSE), color = "blue")
ggplot(lc) +
geom_line(aes(x = size, y = TrainRMSE), color = "red") +
geom_line(aes(x = size, y = TestRMSE), color = "blue") +
geom_hline(yintercept = 0.32, color = "black")
lcm
lcm$raw
lcm$callbacks
importance <- xgb.importance(feature_names = colnames(datTrain), model = bst)
importance <- xgb.importance(feature_names = colnames(datTrain), model = lcm)
dim(importance)
head(importance)
importance
importance[1:100,]
plot(importance)
plot(importance$Gain)
plot(-log(importance$Gain))
feat50 = head(importance$Feature, 50)
feat50
feat50_train = datTrain[,c(feat50, "price_doc")]
feat50_test = datTest[,c(feat50, "price_doc")]
lc50 = as.data.frame(matrix(0, nrow = 10, ncol = 3))
colnames(lc50) = c("size", "TrainRMSE", "TestRMSE")
lc50$size = ceiling(2.732^(1:10))
set.seed(934984)
rand_ind = sample(1:nrow(datTrain), nrow(datTrain))
for(i in 1:nrow(lc50)) {
temptrain = feat50_train[rand_ind[1:lc50[i,1]],]
lcm = xgboost(data = as.matrix(temptrain[,-ncol(temptrain)]),
label = temptrain[,ncol(temptrain)],
objective = "reg:linear", silent = 1, nthread = 8,
eval_metric = "rmse", nrounds = 2000, max_depth = 5,
eta = 0.02, gamma = 0.1, colsample_bytree = 1,
min_child_weight = 1, subsample = 0.7)
lc50[i,2] = ef(predict(lcm, as.matrix(temptrain)),  temptrain$price_doc)
lc50[i,3] = ef(predict(lcm, as.matrix(feat50_test)),  feat50_test$price_doc)
}
ggplot(lc50) +
geom_line(aes(x = size, y = TrainRMSE), color = "red") +
geom_line(aes(x = size, y = TestRMSE), color = "blue") +
geom_hline(yintercept = 0.32, color = "black")
ggplot(lc) +
geom_line(aes(x = size, y = TrainRMSE), color = "red") +
geom_line(aes(x = size, y = TestRMSE), color = "blue") +
geom_hline(yintercept = 0.32, color = "black")
lc50
lc
